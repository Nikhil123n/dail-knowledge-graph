"""
Step 4 (optional): Export clean DAIL data to a fully SQL-ready SQLite database
and generate a portable schema.sql file.

Run after convert_xlsx.py has produced the clean CSVs:
    python -m app.ingest.export_sql   (from backend/ directory)

Outputs (written to data/):
    dail.db       — self-contained SQLite database (portable, no server needed)
    schema.sql    — CREATE TABLE statements for any SQL engine (Postgres, MySQL, SQLite)
    data.sql      — INSERT statements for the four core tables

This makes the pipeline SQL-ready: any SQL database can ingest schema.sql + data.sql
directly, independent of Neo4j.
"""

import os
import sqlite3
import pandas as pd

DATA_DIR = os.path.normpath(
    os.path.join(os.path.dirname(os.path.abspath(__file__)), "..", "..", "..", "data")
)

# ── SQL Schema ─────────────────────────────────────────────────────────────

SCHEMA_SQL = """\
-- DAIL Living Case Graph — SQL Schema
-- Compatible with SQLite, PostgreSQL, and MySQL.
-- Generated by export_sql.py from the DAIL Excel source files.

CREATE TABLE IF NOT EXISTS cases (
    id                      TEXT        PRIMARY KEY,   -- URL-safe slug (Case_snug)
    record_number           TEXT,
    caption                 TEXT        NOT NULL,
    brief_description       TEXT,
    area_of_application     TEXT,                      -- pipe-separated list
    cause_of_action         TEXT,                      -- pipe-separated list
    issues                  TEXT,                      -- pipe-separated list
    algorithm_names         TEXT,                      -- pipe-separated list
    organizations_involved  TEXT,
    jurisdiction_filed      TEXT,
    date_filed              TEXT,                      -- ISO 8601: YYYY-MM-DD
    current_jurisdiction    TEXT,
    jurisdiction_type       TEXT,
    status                  TEXT,                      -- Title-cased (Active, Settled, etc.)
    summary_significance    TEXT,
    summary_facts           TEXT,
    most_recent_activity    TEXT,
    is_class_action         TEXT,
    date_added              TEXT,                      -- ISO 8601: YYYY-MM-DD
    source                  TEXT        DEFAULT 'dail'
);

CREATE TABLE IF NOT EXISTS dockets (
    id          TEXT        PRIMARY KEY,
    case_id     TEXT        NOT NULL REFERENCES cases(id),
    court       TEXT,
    number      TEXT,
    link        TEXT
);

CREATE TABLE IF NOT EXISTS documents (
    id                TEXT    PRIMARY KEY,
    case_id           TEXT    NOT NULL REFERENCES cases(id),
    court             TEXT,
    date              TEXT,   -- ISO 8601
    link              TEXT,
    document_type     TEXT,
    cite_or_reference TEXT
);

CREATE TABLE IF NOT EXISTS secondary_sources (
    id      INTEGER PRIMARY KEY,
    case_id TEXT    NOT NULL REFERENCES cases(id),
    title   TEXT,
    link    TEXT    NOT NULL
);

-- Indexes for common query patterns
CREATE INDEX IF NOT EXISTS idx_cases_status          ON cases(status);
CREATE INDEX IF NOT EXISTS idx_cases_date_filed      ON cases(date_filed);
CREATE INDEX IF NOT EXISTS idx_cases_jurisdiction    ON cases(jurisdiction_filed);
CREATE INDEX IF NOT EXISTS idx_dockets_case_id       ON dockets(case_id);
CREATE INDEX IF NOT EXISTS idx_documents_case_id     ON documents(case_id);
CREATE INDEX IF NOT EXISTS idx_secondary_case_id     ON secondary_sources(case_id);
"""

# ── Helpers ────────────────────────────────────────────────────────────────

def _pipe(val) -> str:
    """Convert a CSV list string or NaN into a pipe-separated string."""
    try:
        if pd.isna(val):
            return ""
    except Exception:
        pass
    s = str(val).strip()
    # Already comma-separated in source — normalise to pipe for SQL storage
    parts = [p.strip() for p in s.split(",") if p.strip() and p.strip().lower() not in ("nan", "none")]
    return "|".join(parts)


def _str(val) -> str:
    try:
        if pd.isna(val):
            return ""
    except Exception:
        pass
    return str(val).strip()


def _escape(val: str) -> str:
    """Escape single quotes for SQL INSERT statements."""
    return val.replace("'", "''")

# ── Loaders ────────────────────────────────────────────────────────────────

def load_cases(conn: sqlite3.Connection) -> pd.DataFrame:
    path = os.path.join(DATA_DIR, "dail_cases.csv")
    df = pd.read_csv(path)
    rows = []
    for _, row in df.iterrows():
        rows.append((
            _str(row.get("Case_snug")),
            _str(row.get("Record_Number")),
            _str(row.get("Caption")),
            _str(row.get("Brief_Description")),
            _pipe(row.get("Area_of_Application_Text")),
            _pipe(row.get("Cause_of_Action_Text")),
            _pipe(row.get("Issue_Text")),
            _pipe(row.get("Name_of_Algorithm_Text")),
            _str(row.get("Organizations_involved")),
            _str(row.get("Jurisdiction_Filed")),
            _str(row.get("Date_Action_Filed")),
            _str(row.get("Current_Jurisdiction")),
            _str(row.get("Jurisdiction_Type_Text")),
            _str(row.get("Status_Disposition")),
            _str(row.get("Summary_of_Significance")),
            _str(row.get("Summary_Facts_Activity_to_Date")),
            _str(row.get("Most_Recent_Activity")),
            _str(row.get("Class_Action")),
            _str(row.get("Date_Added")),
            "dail",
        ))
    conn.executemany("""
        INSERT OR IGNORE INTO cases (
            id, record_number, caption, brief_description,
            area_of_application, cause_of_action, issues, algorithm_names,
            organizations_involved, jurisdiction_filed, date_filed,
            current_jurisdiction, jurisdiction_type, status,
            summary_significance, summary_facts, most_recent_activity,
            is_class_action, date_added, source
        ) VALUES (?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?)
    """, rows)
    print(f"  Cases loaded: {len(rows)}")
    return df


def load_dockets(conn: sqlite3.Connection, cases_df: pd.DataFrame):
    path = os.path.join(DATA_DIR, "dail_dockets.csv")
    if not os.path.exists(path):
        print("  dail_dockets.csv not found, skipping.")
        return
    df = pd.read_csv(path)
    id_to_slug = dict(zip(cases_df["id"].astype(int), cases_df["Case_snug"]))
    rows = []
    for _, row in df.iterrows():
        try:
            case_num = int(float(_str(row.get("Case_Number"))))
        except (ValueError, TypeError):
            continue
        slug = id_to_slug.get(case_num)
        if not slug:
            continue
        rows.append((
            _str(row.get("id")),
            slug,
            _str(row.get("court")),
            _str(row.get("number")),
            _str(row.get("link")),
        ))
    conn.executemany(
        "INSERT OR IGNORE INTO dockets (id, case_id, court, number, link) VALUES (?,?,?,?,?)",
        rows,
    )
    print(f"  Dockets loaded: {len(rows)}")


def load_documents(conn: sqlite3.Connection, cases_df: pd.DataFrame):
    path = os.path.join(DATA_DIR, "dail_documents.csv")
    if not os.path.exists(path):
        print("  dail_documents.csv not found, skipping.")
        return
    df = pd.read_csv(path)
    id_to_slug = dict(zip(cases_df["id"].astype(int), cases_df["Case_snug"]))
    rows = []
    for _, row in df.iterrows():
        try:
            case_num = int(float(_str(row.get("Case_Number"))))
        except (ValueError, TypeError):
            continue
        slug = id_to_slug.get(case_num)
        if not slug:
            continue
        rows.append((
            _str(row.get("id")),
            slug,
            _str(row.get("court")),
            _str(row.get("date")),
            _str(row.get("link")),
            _str(row.get("document")),
            _str(row.get("cite_or_reference")),
        ))
    conn.executemany(
        "INSERT OR IGNORE INTO documents (id, case_id, court, date, link, document_type, cite_or_reference) VALUES (?,?,?,?,?,?,?)",
        rows,
    )
    print(f"  Documents loaded: {len(rows)}")


def load_secondary_sources(conn: sqlite3.Connection, cases_df: pd.DataFrame):
    path = os.path.join(DATA_DIR, "dail_secondary_sources.csv")
    if not os.path.exists(path):
        print("  dail_secondary_sources.csv not found, skipping.")
        return
    df = pd.read_csv(path)
    id_to_slug = dict(zip(cases_df["id"].astype(int), cases_df["Case_snug"]))
    rows = []
    for _, row in df.iterrows():
        try:
            case_num = int(float(_str(row.get("Case_Number"))))
        except (ValueError, TypeError):
            continue
        slug = id_to_slug.get(case_num)
        if not slug:
            continue
        link = _str(row.get("Secondary_Source_Link"))
        if not link or link in ("nan", ""):
            continue
        rows.append((
            int(row.get("id", 0)),
            slug,
            _str(row.get("Secondary_Source_Title")),
            link,
        ))
    conn.executemany(
        "INSERT OR IGNORE INTO secondary_sources (id, case_id, title, link) VALUES (?,?,?,?)",
        rows,
    )
    print(f"  Secondary sources loaded: {len(rows)}")


def export_data_sql(conn: sqlite3.Connection):
    """Write a data.sql file with INSERT statements for all four tables."""
    out_path = os.path.join(DATA_DIR, "data.sql")
    tables = ["cases", "dockets", "documents", "secondary_sources"]
    with open(out_path, "w", encoding="utf-8") as f:
        f.write("-- DAIL Living Case Graph — Data Export\n")
        f.write("-- Generated by export_sql.py\n\n")
        for table in tables:
            cursor = conn.execute(f"SELECT * FROM {table}")  # noqa: S608
            cols = [d[0] for d in cursor.description]
            rows = cursor.fetchall()
            f.write(f"-- {table}: {len(rows)} rows\n")
            for row in rows:
                vals = ", ".join(
                    f"'{_escape(str(v))}'" if v is not None else "NULL"
                    for v in row
                )
                f.write(f"INSERT OR IGNORE INTO {table} ({', '.join(cols)}) VALUES ({vals});\n")
            f.write("\n")
    print(f"  data.sql written ({os.path.getsize(out_path) // 1024} KB)")


# ── Main ───────────────────────────────────────────────────────────────────

def main():
    # Verify clean CSVs exist
    required = ["dail_cases.csv"]
    missing = [f for f in required if not os.path.exists(os.path.join(DATA_DIR, f))]
    if missing:
        print("ERROR: Run convert_xlsx.py first to generate clean CSVs.")
        print("  python -m app.ingest.convert_xlsx")
        raise SystemExit(1)

    # Write schema.sql
    schema_path = os.path.join(DATA_DIR, "schema.sql")
    with open(schema_path, "w", encoding="utf-8") as f:
        f.write(SCHEMA_SQL)
    print(f"schema.sql written → {schema_path}")

    # Build SQLite database
    db_path = os.path.join(DATA_DIR, "dail.db")
    if os.path.exists(db_path):
        os.remove(db_path)
    conn = sqlite3.connect(db_path)
    conn.executescript(SCHEMA_SQL)

    print("\nLoading tables into dail.db ...")
    cases_df = load_cases(conn)
    load_dockets(conn, cases_df)
    load_documents(conn, cases_df)
    load_secondary_sources(conn, cases_df)
    conn.commit()

    # Summary
    print("\nDatabase summary:")
    for table in ["cases", "dockets", "documents", "secondary_sources"]:
        count = conn.execute(f"SELECT COUNT(*) FROM {table}").fetchone()[0]  # noqa: S608
        print(f"  {table:<25} {count:>5} rows")

    # Export data.sql
    print("\nExporting data.sql ...")
    export_data_sql(conn)
    conn.close()

    print(f"\nSQL pipeline complete. Outputs written to data/:")
    print(f"  dail.db        — SQLite database (open with DB Browser for SQLite or any SQL tool)")
    print(f"  schema.sql     — CREATE TABLE statements (compatible with PostgreSQL / MySQL / SQLite)")
    print(f"  data.sql       — INSERT statements for all four tables")
    print(f"\nTo load into PostgreSQL:")
    print(f"  psql -d your_db -f data/schema.sql")
    print(f"  psql -d your_db -f data/data.sql")


if __name__ == "__main__":
    main()
